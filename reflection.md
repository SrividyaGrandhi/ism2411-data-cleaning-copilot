For this project I used GitHub Copilot mainly to help write the functions in my data_cleaning.py script. What I did was write a short comment above each function saying what I needed it to do, and then I would start typing the function definition. Copilot would pop up with a suggestion. For example, when I wrote the comment about loading the data and started typing def load_data, Copilot suggested code using pd.read_csv() which was exactly what I needed. Same thing happened for clean_column_names, handle_missing_values, and remove_invalid_rows. I got a decent starting point just from explaining what I wanted.

But I definitely did not keep Copilot’s code exactly as it gave it to me. I had to change a bunch of stuff to actually make the cleaning work. For example, Copilot didn’t know my dataset used qty instead of quantity, so its code broke until I fixed that. Also, my text data had a bunch of extra quotes like """Office""", which Copilot didn’t clean out, so I added .str.replace('"', '', regex=False) to remove those. In handle_missing_values, I also changed the logic so it converted the columns to numeric before dropping missing values — otherwise I got errors when comparing strings to numbers. I ended up adding comments explaining why each cleaning step mattered, like removing negative values because they don’t make sense in a sales dataset.

What I learned from this is that data cleaning is a lot more detailed than I expected. There were all kinds of small issues like spacing, weird quotes, wrong data types, and missing values that had to be handled before the data actually looked clean. And with Copilot, I learned that it’s really good at giving you a quick starting point, but it doesn’t automatically understand your exact assignment or your exact data. You have to review its suggestions, fix mistakes, and add your own logic. It saved me time, but it definitely didn’t do the project for me. I still had to think through why each step was needed and make changes so everything would run correctly.